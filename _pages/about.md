---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>
I am joining [Nanyang Technological University](https://www.ntu.edu.sg/) as a Research Fellow. I completed my PhD at the [National Engineering Research Center of Robot Visual Perception and Control Technology](http://robot.hnu.edu.cn/) of [Hunan University](https://www.hnu.edu.cn/), supervised by [Prof. Wei Sun](https://eeit.hnu.edu.cn/info/1281/4511.htm). From Aug. 2023 to Sep. 2024, I was a Visiting PhD Student at the Machine Intelligence Group of the [University of Western Australia](https://www.uwa.edu.au/), supervised by [Prof. Ajmal Mian](https://ajmalsaeed.net/). I also work closely with [Prof. Nicu Sebe](http://disi.unitn.it/~sebe/) and [Prof. Hossein Rahmani](https://sites.google.com/view/rahmaniatlu/).

My research focuses on 3D machine vision and its applications for robotic manipulation. Specifically, I have worked on object pose estimation and tracking. Subsequent research focuses include label-efficient learning for generalized robotic multimodal perception, manipulation, and embodied AI. I was motivated to conduct this research due to my passion for realizing generalizable perception and manipulation of robots in 3D physical space.

# 🔥 News
- *2025.03*: &nbsp;🎉🎉 One paper get accepted by IEEE TCyber.
- *2025.02*: &nbsp;🎉🎉 One paper get accepted by IEEE ICRA'25.
- *2024.12*: &nbsp;🎉🎉 PhD defended.
- *2024.09*: &nbsp;🎉🎉 One paper get accepted by IEEE IoT-J.
- *2024.05*: &nbsp;🎉🎉 A comprehensive survey of deep learning-based object pose estimation was posted on [arXiv](https://arxiv.org/pdf/2405.07801v3). Feel free to contact us if you have any suggestions.
- *2024.02*: &nbsp;🎉🎉 One paper get accepted by IEEE TII.
- *2024.01*: &nbsp;🎉🎉 One paper get accepted by IEEE TNNLS.
- *2024.01*: &nbsp;🎉🎉 One paper get accepted by IEEE TMC.
- *2023.11*: &nbsp;🎉🎉 One paper get accepted by IEEE TIM.
- *2023.02*: &nbsp;🎉🎉 One paper get accepted by IEEE TII.
- *2022.06*: &nbsp;🎉🎉 One paper get accepted by IEEE TCSVT.
  
# 📝 Selected Publications
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/paper6_512x512.png' alt="sym" width="100%">
</div></div>
<div class='paper-box-text' markdown="1">
[Diff9D: Diffusion-Based Domain-Generalized Category-Level 9DoF Object Pose Estimation](https://arxiv.org/abs/2502.02525) [**Code**](https://github.com/CNJianLiu/Diff9D)\\
**Jian Liu**, Wei Sun, Hui Yang, Pengchao Deng, Chongpei Liu, Nicu Sebe, Hossein Rahmani, Ajmal Mian
- We propose an effective diffusion model to redefine 9DoF object pose estimation from a generative perspective. Diff9D is a simple yet effective prior-free domain-generalized (sim2real) category-level 9DoF object pose generator. By employing the denoising diffusion implicit model, we demonstrate that the reverse diffusion process can be executed in as few as 3 steps, achieving near real-time performance.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/paper5_512x512.png' alt="sym" width="100%">
</div></div>
<div class='paper-box-text' markdown="1">
[Novel Object 6D Pose Estimation with a Single Reference View](https://arxiv.org/abs/2503.05578) [**Code**](https://github.com/CNJianLiu/SinRef-6D)\\
**Jian Liu**, Wei Sun, Kai Zeng, Jin Zheng, Hui Yang, Lin Wang, Hossein Rahmani, Ajmal Mian
- We propose a single reference view-based CAD model-free novel object 6D pose estimation method. SinRef-6D is simple yet effective and can simultaneously eliminate the need for object CAD models, dense reference views, and model retraining, offering enhanced efficiency and scalability while demonstrating strong generalization to potential real-world robotic applications.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='images/paper4_512x512.png' alt="sym" width="100%">
</div></div>
<div class='paper-box-text' markdown="1">
[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](https://arxiv.org/pdf/2405.07801v3) [**Project Page**](https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation)\\
**Jian Liu**, Wei Sun, Hui Yang, Zhiwen Zeng, Chongpei Liu, Jin Zheng, Xingyu Liu, Hossein Rahmani, Nicu Sebe, Ajmal Mian
- We present a comprehensive survey of deep learning-based object pose estimation methods. This survey covers all three problem formulations in the domain, including instance-level, category-level, and unseen object pose estimation. We hope to provide readers with a complete picture of the research progress of deep learning-based object pose estimation.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TNNLS 2024</div><img src='images/paper3_512x512.png' alt="sym" width="100%">
</div></div>
<div class='paper-box-text' markdown="1">
[MH6D: Multi-Hypothesis Consistency Learning for Category-Level 6D Object Pose Estimation](https://ieeexplore.ieee.org/document/10433529) [**Code**](https://github.com/CNJianLiu/MH6D)\\
**Jian Liu**, Wei Sun, Chongpei Liu, Hui Yang, Xing Zhang, Ajmal Mian
- We propose a multi-hypothesis consistency learning framework for category-level 6D object pose estimation, which utilizes a parallel consistency learning structure, alleviating the uncertainty problem of single-shot feature extraction and promoting self-adaptation of domain to reduce the synthetic-to-real domain gap.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TII 2023</div><img src='images/paper2_512x512.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Robotic Continuous Grasping System by Shape Transformer-Guided Multi-Object Category-Level 6D Pose Estimation](https://ieeexplore.ieee.org/abstract/document/10043016) [**Code**](https://github.com/CNJianLiu/6D-CLGrasp)\\
**Jian Liu**, Wei Sun, Chongpei Liu, Xing Zhang, Qiang Fu
- A transformer-guided shape reconstruction network is proposed to reconstruct the NOCS shape of intra-class known objects, which can fully use the prior feature, current observation feature, and their feature difference by internal self-attention, as well as strengthen their correlation by mutual cross-attention. By doing so, the shape variation can be explicitly highlighted.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TCSVT 2022</div><img src='images/paper1_512x512.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[HFF6D: Hierarchical Feature Fusion Network for Robust 6D Object Pose Tracking](https://ieeexplore.ieee.org/abstract/document/9792223)\\
**Jian Liu**, Wei Sun, Chongpei Liu, Xing Zhang, Shimeng Fan, Wei Wu
- We propose a lightweight and robust hierarchical feature fusion network for 6D object pose tracking. It establishes sufficient spatial-temporal information interaction between adjacent frames and explicitly highlights the feature differences between adjacent frames, thus improving the robustness of relative pose estimation in challenging scenes.
</div></div>
 
# 💻 Real-World Robotic Projects
<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><img src='images/project1_512x512.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[**Robotic Continuous Grasping System**](https://github.com/CNJianLiu/6D-CLGrasp) (Demo can be seen through [link1](https://www.bilibili.com/video/BV16M4y1Q7CD) or [link2](https://youtu.be/ZeGN6_DChuA))
- We build an end-to-end robotic continuous grasping system, which achieves high-accuracy 6D pose estimation for multiple intra-class unknown objects and high-efficiency robotic grasping in 3D space. For continuous grasping, we propose a low-computation and effective grasping strategy based on the pre-defined vector orientation, and develop a GUI for monitoring and control.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><img src='images/project2_512x512.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[**Robotic Continuous Picking System**](https://github.com/CNJianLiu/Diff9D) (Demo can be seen through [link](https://youtu.be/D4bV8eIUvWk))
- We develop an object pose-guided robotic picking system comprising both hardware and software components. The hardware for the robotic picking system is composed of an Intel RealSense L515 RGB-D camera, a Yaskawa robot MOTOMAN-MH12, an electric parallel gripper DH-PGI140-80, and a host computer. For software, we develop a GUI comprising three parts: calibration module, server-client TCP communication module, and robotic picking module.
</div></div>

# 🎖 Honors and Awards
- *2024.11* Ph.D. National Scholarship.
- *2018.11* The National First Prize in "Higher Education Society Cup" National Undergraduate Mathematical Contest in Modeling (Top 1.5 %).
- *2019.08* The National Second Prize in "RoboMaster2019" National Undergraduate Robotics Competition (Hosted by [DJI-Innovations](https://www.robomaster.com/en-US)).
- *2018.06* Hong Kong "Zhong Huiming" Social Scholarship.

# 📖 Review Services
 I serve as a reviewer for more than ten journals/conferences, mainly including:
- IEEE Transactions on Pattern Analysis and Machine Intelligence
- IEEE Transactions on Image Processing
- IEEE Transactions on Neural Networks and Learning Systems
- IEEE Transactions on Industrial Informatics
- IEEE Transactions on Circuits and Systems for Video Technology
- IEEE Transactions on Circuits and Systems I: Regular Papers
- IEEE Transactions on Instrumentation and Measurement
- Pattern Recognition
- IEEE/CVF International Conference on Computer Vision (ICCV)
- IEEE International Conference on Robotics and Automation (ICRA)

<!-- # 📖 Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<!-- # 💬 Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

<!-- # 💻 Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->

<!-- {% include_relative includes/news.md %}

{% include_relative includes/pub.md %}

{% include_relative includes/honers.md %}

{% include_relative includes/others.md %}
 -->
