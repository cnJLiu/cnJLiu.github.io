---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span class='anchor' id='about-me'></span>
I recently completed my PhD at the [National Engineering Research Center of Robot Visual Perception and Control Technology](http://robot.hnu.edu.cn/) of [Hunan University](https://www.hnu.edu.cn/), supervised by [Prof. Wei Sun](http://robotics.hnu.edu.cn/info/1176/2972.htm) and [Prof. Yaonan Wang](https://robotics.hnu.edu.cn/info/1176/3098.htm). From Aug. 2023 to Sep. 2024, I was a Visiting PhD Student at the Machine Intelligence Group of the [University of Western Australia](https://www.uwa.edu.au/), supervised by [Prof. Ajmal Mian](https://ajmalsaeed.net/). I also work closely with [Prof. Nicu Sebe](http://disi.unitn.it/~sebe/) and [Prof. Hossein Rahmani](https://sites.google.com/view/rahmaniatlu/).

My PhD research focuses on 3D machine vision and its applications for robotic manipulation. Subsequent research focuses include label-efficient learning for generalized robotic multimodal perception and embodied AI. I was motivated to conduct this research due to my passion for realizing generalizable perception, navigation, and manipulation of robots in 3D physical space.

[*If you have excellent research for collaboration, feel free to get in touch!*](mailto:jianliu666.cn@gmail.com)

# ğŸ”¥ News
- *2025.10*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IJCV!
- *2025.06*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE RAL.
- *2025.05*: &nbsp;ğŸ‰ğŸ‰ We proposed a computation framework called [Neural Brain](https://arxiv.org/abs/2505.07634) for embodied agents through the lens of neuroscience, providing an innovative research perspective for embodied AI. Feel free to get in touch if you have any ideas.
- *2025.04*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE RAL.
- *2025.03*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE TPAMI!
- *2025.03*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE TCyber.
- *2025.02*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE ICRA'25.
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE IoT-J.
- *2024.05*: &nbsp;ğŸ‰ğŸ‰ A comprehensive survey of deep learning-based object pose estimation was posted on [arXiv](https://arxiv.org/pdf/2405.07801v3). Feel free to contact us if you have any suggestions.
- *2024.02*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE TII.
- *2024.01*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE TNNLS.
- *2024.01*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE TMC.
- *2023.11*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE TIM.
- *2023.02*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE TII.
- *2022.06*: &nbsp;ğŸ‰ğŸ‰ One paper get accepted by IEEE TCSVT.
  
# ğŸ“ Selected Publications
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TPAMI 2025</div><img src='images/paper6_512x512.png' alt="sym" width="100%">
</div></div>
<div class='paper-box-text' markdown="1">
[Diff9D: Diffusion-Based Domain-Generalized Category-Level 9DoF Object Pose Estimation](https://ieeexplore.ieee.org/document/10930708) ([**Code**](https://github.com/CNJianLiu/Diff9D))\\
**Jian Liu**, Wei Sun, Hui Yang, Pengchao Deng, Chongpei Liu, Nicu Sebe, Hossein Rahmani, Ajmal Mian
- We propose an effective diffusion model to redefine 9DoF object pose estimation from a generative perspective. Diff9D is a simple yet effective prior-free domain-generalized (sim2real) category-level 9DoF object pose generator. By employing the denoising diffusion implicit model, we demonstrate that the reverse diffusion process can be executed in as few as 3 steps, achieving near real-time performance.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCV 2025</div><img src='images/paper4_512x512.png' alt="sym" width="100%">
</div></div>
<div class='paper-box-text' markdown="1">
[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](https://arxiv.org/pdf/2405.07801v3)\\
([**Project Page**](https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation))\\
**Jian Liu**, Wei Sun, Hui Yang, Zhiwen Zeng, Chongpei Liu, Jin Zheng, Xingyu Liu, Hossein Rahmani, Nicu Sebe, Ajmal Mian
- We present a comprehensive survey of deep learning-based object pose estimation methods. This survey covers all three problem formulations in the domain, including instance-level, category-level, and unseen object pose estimation. We hope to provide readers with a complete picture of the research progress of deep learning-based object pose estimation.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/paper5_512x512.png' alt="sym" width="100%">
</div></div>
<div class='paper-box-text' markdown="1">
[Novel Object 6D Pose Estimation with a Single Reference View](https://arxiv.org/abs/2503.05578v3) ([**Code**](https://github.com/CNJianLiu/SinRef-6D))\\
**Jian Liu**, Wei Sun, Kai Zeng, Jin Zheng, Hui Yang, Hossein Rahmani, Ajmal Mian, Lin Wang
- We propose a single reference view-based CAD model-free novel object 6D pose estimation method. SinRef-6D is simple yet effective and can simultaneously eliminate the need for object CAD models, dense reference views, and model retraining, offering enhanced efficiency and scalability while demonstrating strong generalization to potential real-world robotic applications.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE ICRA'25</div><img src='images/paper7_512x512.png' alt="sym" width="100%">
</div></div>
<div class='paper-box-text' markdown="1">
[MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via Diffusion Model](https://ieeexplore.ieee.org/document/11127837) ([**Code**](https://github.com/CNJianLiu/MonoDiff9D))\\
**Jian Liu**, Wei Sun, Hui Yang, Jin Zheng, Zichen Geng, Hossein Rahmani, Ajmal Mian
- MonoDiff9D is an extension of [Diff9D](https://ieeexplore.ieee.org/document/10930708), aiming to achieve monocular category-level 9D object pose estimation via diffusion model conditioning on large vision model-based zero-shot depth recovery, without the need for shape priors or CAD models at any stage.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TNNLS 2024</div><img src='images/paper3_512x512.png' alt="sym" width="100%">
</div></div>
<div class='paper-box-text' markdown="1">
[MH6D: Multi-Hypothesis Consistency Learning for Category-Level 6D Object Pose Estimation](https://ieeexplore.ieee.org/document/10433529) ([**Code**](https://github.com/CNJianLiu/MH6D))\\
**Jian Liu**, Wei Sun, Chongpei Liu, Hui Yang, Xing Zhang, Ajmal Mian
- We propose a multi-hypothesis consistency learning framework for category-level 6D object pose estimation, which utilizes a parallel consistency learning structure, alleviating the uncertainty problem of single-shot feature extraction and promoting self-adaptation of domain to reduce the synthetic-to-real domain gap.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TII 2023</div><img src='images/paper2_512x512.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Robotic Continuous Grasping System by Shape Transformer-Guided Multi-Object Category-Level 6D Pose Estimation](https://ieeexplore.ieee.org/abstract/document/10043016) ([**Code**](https://github.com/CNJianLiu/6D-CLGrasp))\\
**Jian Liu**, Wei Sun, Chongpei Liu, Xing Zhang, Qiang Fu
- A transformer-guided shape reconstruction network is proposed to reconstruct the NOCS shape of intra-class known objects, which can fully use the prior feature, current observation feature, and their feature difference by internal self-attention, as well as strengthen their correlation by mutual cross-attention. By doing so, the shape variation can be explicitly highlighted.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TCSVT 2022</div><img src='images/paper1_512x512.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[HFF6D: Hierarchical Feature Fusion Network for Robust 6D Object Pose Tracking](https://ieeexplore.ieee.org/abstract/document/9792223)\\
**Jian Liu**, Wei Sun, Chongpei Liu, Xing Zhang, Shimeng Fan, Wei Wu
- We propose a lightweight and robust hierarchical feature fusion network for 6D object pose tracking. It establishes sufficient spatial-temporal information interaction between adjacent frames and explicitly highlights the feature differences between adjacent frames, thus improving the robustness of relative pose estimation in challenging scenes.
</div></div>
 
# ğŸ’» Real-World Robotic Projects
<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><img src='images/project1_512x512.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[**Robotic Continuous Grasping System**](https://github.com/CNJianLiu/6D-CLGrasp) (Demo can be seen through [link1](https://www.bilibili.com/video/BV16M4y1Q7CD) or [link2](https://youtu.be/ZeGN6_DChuA))
- We build an end-to-end robotic continuous grasping system, which achieves high-accuracy 6D pose estimation for multiple intra-class unknown objects and high-efficiency robotic grasping in 3D space. For continuous grasping, we propose a low-computation and effective grasping strategy based on the pre-defined vector orientation, and develop a GUI for monitoring and control.
</div></div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><img src='images/project2_512x512.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[**Robotic Continuous Picking System**](https://github.com/CNJianLiu/Diff9D) (Demo can be seen through [link](https://youtu.be/D4bV8eIUvWk))
- We develop an object pose-guided robotic picking system comprising both hardware and software components. The hardware for the robotic picking system is composed of an Intel RealSense L515 RGB-D camera, a Yaskawa robot MOTOMAN-MH12, an electric parallel gripper DH-PGI140-80, and a host computer. For software, we develop a GUI comprising three parts: calibration module, server-client TCP communication module, and robotic picking module.
</div></div>

# ğŸ– Honors and Awards
- *2024.11* Ph.D. National Scholarship.
- *2018.11* The National First Prize in "Higher Education Society Cup" National Undergraduate Mathematical Contest in Modeling (Top 1.5 %).
- *2019.08* The National Second Prize in "RoboMaster2019" National Undergraduate Robotics Competition (Hosted by [DJI-Innovations](https://www.robomaster.com/en-US)).
- *2018.06* Hong Kong "Zhong Huiming" Social Scholarship.

# ğŸ“– Editorial and Review Services
Leading Guest Editor: [Electronics](https://www.mdpi.com/journal/electronics), Special Issue on "[Recent Advances in Robot Manipulation](https://www.mdpi.com/journal/electronics/special_issues/NAU56Y505W)"

I serve as a reviewer for more than 20 journals/conferences, mainly including:
- IEEE Transactions on Pattern Analysis and Machine Intelligence
- IEEE Transactions on Image Processing
- IEEE Transactions on Neural Networks and Learning Systems
- IEEE Transactions on Industrial Informatics
- IEEE Transactions on Circuits and Systems for Video Technology
- IEEE/ASME Transactions on Mechatronics
- IEEE Transactions on Automation Science and Engineering
- IEEE Transactions on Systems, Man, and Cybernetics: Systems
- IEEE Transactions on Circuits and Systems I: Regular Papers
- IEEE Transactions on Instrumentation and Measurement
- IEEE Robotics and Automation Letters
- ACM Computing Surveys
- Pattern Recognition
- Neural Networks
- IEEE/CVF International Conference on Computer Vision (ICCV)
- IEEE International Conference on Robotics and Automation (ICRA)
- IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)

<!-- # ğŸ“– Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<!-- # ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

<!-- # ğŸ’» Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->

<!-- {% include_relative includes/news.md %}

{% include_relative includes/pub.md %}
- *2025.05*: &nbsp;ğŸ‰ğŸ‰ We proposed a computation framework called [Neural Brain](https://arxiv.org/abs/2505.07634) for embodied agents through the lens of neuroscience, providing an innovative research perspective for embodied AI. Feel free to get in touch if you have any ideas.
{% include_relative includes/honers.md %}

{% include_relative includes/others.md %}
 -->
